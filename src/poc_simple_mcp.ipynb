{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e520ff4",
   "metadata": {},
   "source": [
    "# MCP (Model Context Protocol) - Esempio Minimale con Ollama\n",
    "\n",
    "Questo notebook dimostra l'utilizzo di MCP con un server locale Ollama usando il modello llama3.2:1b.\n",
    "\n",
    "## Prerequisiti\n",
    "- Ollama installato e in esecuzione\n",
    "- Modello llama3.2:1b scaricato\n",
    "- Librerie Python: `mcp`, `ollama`, `asyncio`\n",
    "\n",
    "**IMPORTANTE**: Su Windows, assicurati di aver attivato il venv Python:\n",
    "```bash\n",
    "source .venv/Scripts/activate.ps1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ebdc847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tutte le dipendenze sono installate!\n"
     ]
    }
   ],
   "source": [
    "# Installazione delle dipendenze necessarie\n",
    "# Eseguire solo se le librerie non sono gi√† installate\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        print(f\"Installazione di {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"‚úÖ {package} installato!\")\n",
    "\n",
    "# Lista delle dipendenze\n",
    "dependencies = [\"ollama\", \"mcp\", \"pydantic\"]\n",
    "\n",
    "for dep in dependencies:\n",
    "    install_package(dep)\n",
    "\n",
    "print(\"‚úÖ Tutte le dipendenze sono installate!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29fa3ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import subprocess\n",
    "from typing import Any, Dict, List\n",
    "import ollama\n",
    "from mcp import types\n",
    "from mcp.server import Server\n",
    "from mcp.client.session import ClientSession\n",
    "from mcp.client.stdio import stdio_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815043ed",
   "metadata": {},
   "source": [
    "## 1. Configurazione Ollama\n",
    "\n",
    "Verifichiamo che Ollama sia in esecuzione e che il modello llama3.2:1b sia disponibile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b74018e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama √® in esecuzione!\n",
      "Risposta completa: models=[Model(model='gemma3:270m', modified_at=datetime.datetime(2025, 9, 3, 10, 31, 25, 795477, tzinfo=TzInfo(UTC)), digest='e7d36fb2c3b3293cfe56d55889867a064b3a2b22e98335f2e6e8a387e081d6be', size=291554930, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='268.10M', quantization_level='Q8_0'))]\n",
      "Modelli disponibili: ['gemma3:270m']\n",
      "‚ö†Ô∏è Modello llama3.2:1b non trovato. Scaricamento in corso...\n",
      "‚úÖ Modello llama3.2:1b scaricato!\n"
     ]
    }
   ],
   "source": [
    "# Verifica disponibilit√† Ollama e modello\n",
    "try:\n",
    "    # Controllo se Ollama √® in esecuzione\n",
    "    models_response = ollama.list()\n",
    "    print(\"Ollama √® in esecuzione!\")\n",
    "    print(f\"Risposta completa: {models_response}\")\n",
    "    \n",
    "    # Estrazione sicura dei modelli\n",
    "    if 'models' in models_response:\n",
    "        models_list = models_response['models']\n",
    "        model_names = []\n",
    "        \n",
    "        for model in models_list:\n",
    "            # Gestione di diverse strutture dati possibili\n",
    "            if isinstance(model, ollama.ListResponse.Model):\n",
    "                if 'name' in model:\n",
    "                    model_names.append(model['name'])\n",
    "                elif 'model' in model:\n",
    "                    model_names.append(model['model'])\n",
    "            elif isinstance(model, str):\n",
    "                model_names.append(model)\n",
    "        \n",
    "        print(f\"Modelli disponibili: {model_names}\")\n",
    "        \n",
    "        # Verifica se llama3.2:1b √® disponibile\n",
    "        llama_found = any('llama3.2:1b' in name for name in model_names)\n",
    "        if llama_found:\n",
    "            print(\"‚úÖ Modello llama3.2:1b trovato!\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Modello llama3.2:1b non trovato. Scaricamento in corso...\")\n",
    "            try:\n",
    "                ollama.pull('llama3.2:1b')\n",
    "                print(\"‚úÖ Modello llama3.2:1b scaricato!\")\n",
    "            except Exception as pull_error:\n",
    "                print(f\"‚ùå Errore durante il download: {pull_error}\")\n",
    "                print(\"Prova a scaricare manualmente con: ollama pull llama3.2:1b\")\n",
    "    else:\n",
    "        print(\"‚ùå Struttura risposta inaspettata da Ollama\")\n",
    "        print(\"Prova a eseguire 'ollama list' dal terminale per verificare\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Errore con Ollama: {e}\")\n",
    "    print(f\"Tipo errore: {type(e)}\")\n",
    "    print(\"Possibili soluzioni:\")\n",
    "    print(\"1. Verifica che Ollama sia installato: ollama --version\")\n",
    "    print(\"2. Avvia il servizio Ollama se non √® in esecuzione\")\n",
    "    print(\"3. Su Windows: verifica che il servizio Ollama sia attivo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3850001f",
   "metadata": {},
   "source": [
    "## 2. Creazione MCP Server\n",
    "\n",
    "Creiamo un server MCP semplice con alcuni tool di esempio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79d6f5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MCP Server creato con successo!\n"
     ]
    }
   ],
   "source": [
    "class SimpleMCPServer:\n",
    "    def __init__(self):\n",
    "        self.server = Server(\"simple-mcp-server\")\n",
    "        self.tools_list = []\n",
    "        self.setup_tools()\n",
    "    \n",
    "    def setup_tools(self):\n",
    "        # Definiamo i tool disponibili\n",
    "        self.tools_list = [\n",
    "            types.Tool(\n",
    "                name=\"calculator\",\n",
    "                description=\"Esegue calcoli matematici semplici\",\n",
    "                inputSchema={\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"expression\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Espressione matematica da calcolare\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"expression\"]\n",
    "                }\n",
    "            ),\n",
    "            types.Tool(\n",
    "                name=\"text_analyzer\",\n",
    "                description=\"Analizza un testo e restituisce statistiche\",\n",
    "                inputSchema={\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"text\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Testo da analizzare\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"text\"]\n",
    "                }\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Registriamo i handler\n",
    "        @self.server.list_tools()\n",
    "        async def handle_list_tools() -> List[types.Tool]:\n",
    "            \"\"\"Lista dei tool disponibili\"\"\"\n",
    "            return self.tools_list\n",
    "        \n",
    "        @self.server.call_tool()\n",
    "        async def handle_call_tool(name: str, arguments: Dict[str, Any]) -> List[types.TextContent]:\n",
    "            \"\"\"Gestisce le chiamate ai tool\"\"\"\n",
    "            return await self.execute_tool(name, arguments)\n",
    "    \n",
    "    async def execute_tool(self, name: str, arguments: Dict[str, Any]) -> List[types.TextContent]:\n",
    "        \"\"\"Esegue il tool specificato\"\"\"\n",
    "        if name == \"calculator\":\n",
    "            try:\n",
    "                expression = arguments[\"expression\"]\n",
    "                # Valutazione sicura di espressioni matematiche semplici\n",
    "                result = eval(expression, {\"__builtins__\": {}}, {\n",
    "                    \"abs\": abs, \"round\": round, \"min\": min, \"max\": max,\n",
    "                    \"sum\": sum, \"pow\": pow, \"len\": len, \"+\": lambda x, y: x + y,\n",
    "                    \"-\": lambda x, y: x - y, \"*\": lambda x, y: x * y, \"/\": lambda x, y: x / y\n",
    "                })\n",
    "                return [types.TextContent(\n",
    "                    type=\"text\",\n",
    "                    text=f\"Risultato: {expression} = {result}\"\n",
    "                )]\n",
    "            except Exception as e:\n",
    "                return [types.TextContent(\n",
    "                    type=\"text\",\n",
    "                    text=f\"Errore nel calcolo: {str(e)}\"\n",
    "                )]\n",
    "        \n",
    "        elif name == \"text_analyzer\":\n",
    "            text = arguments[\"text\"]\n",
    "            words = text.split()\n",
    "            chars = len(text)\n",
    "            sentences = text.count('.') + text.count('!') + text.count('?')\n",
    "            \n",
    "            analysis = {\n",
    "                \"caratteri\": chars,\n",
    "                \"parole\": len(words),\n",
    "                \"frasi\": sentences,\n",
    "                \"caratteri_medi_per_parola\": round(chars / len(words), 2) if words else 0\n",
    "            }\n",
    "            \n",
    "            return [types.TextContent(\n",
    "                type=\"text\",\n",
    "                text=f\"Analisi del testo:\\n{json.dumps(analysis, indent=2, ensure_ascii=False)}\"\n",
    "            )]\n",
    "        \n",
    "        else:\n",
    "            return [types.TextContent(\n",
    "                type=\"text\",\n",
    "                text=f\"Tool '{name}' non riconosciuto\"\n",
    "            )]\n",
    "    \n",
    "    async def get_tools(self):\n",
    "        \"\"\"Restituisce la lista dei tool disponibili\"\"\"\n",
    "        return self.tools_list\n",
    "    \n",
    "    async def call_tool_method(self, name: str, arguments: Dict[str, Any]):\n",
    "        \"\"\"Wrapper per chiamare i tool\"\"\"\n",
    "        return await self.execute_tool(name, arguments)\n",
    "\n",
    "# Creazione del server\n",
    "mcp_server = SimpleMCPServer()\n",
    "print(\"‚úÖ MCP Server creato con successo!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab20cbd6",
   "metadata": {},
   "source": [
    "## 3. Funzione Helper per Ollama\n",
    "\n",
    "Creiamo una funzione per interagire con Ollama in modo semplice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cab1943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test di comunicazione con Ollama...\n",
      "Risposta di Ollama: Ciao! Io sto bene, grazie. Sono felice di poterti aiutare con qualsiasi cosa. Come posso aiutarti oggi?\n"
     ]
    }
   ],
   "source": [
    "def chat_with_ollama(prompt: str, model: str = \"llama3.2:1b\", tools_available: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Interagisce con Ollama usando il modello specificato\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if tools_available:\n",
    "            # Aggiungiamo informazioni sui tool disponibili nel prompt\n",
    "            system_prompt = \"\"\"\n",
    "            Sei un assistente AI con accesso ai seguenti tool:\n",
    "            1. calculator: per calcoli matematici (usa l'argomento 'expression')\n",
    "            2. text_analyzer: per analizzare testi (usa l'argomento 'text')\n",
    "            \n",
    "            Quando l'utente chiede qualcosa che pu√≤ essere risolto con questi tool,\n",
    "            rispondi con il formato: TOOL_CALL: nome_tool {\"argomento\": \"valore\"}\n",
    "            \"\"\"\n",
    "            full_prompt = f\"{system_prompt}\\n\\nUtente: {prompt}\"\n",
    "        else:\n",
    "            full_prompt = prompt\n",
    "            \n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[{'role': 'user', 'content': full_prompt}]\n",
    "        )\n",
    "        return response['message']['content']\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Errore nella comunicazione con Ollama: {str(e)}\"\n",
    "        print(f\"Debug - {error_msg}\")\n",
    "        \n",
    "        # Suggerimenti per errori comuni\n",
    "        if \"model\" in str(e).lower():\n",
    "            error_msg += \"\\nSuggerimento: Verifica che il modello sia disponibile con 'ollama list'\"\n",
    "        elif \"connection\" in str(e).lower():\n",
    "            error_msg += \"\\nSuggerimento: Verifica che Ollama sia in esecuzione\"\n",
    "            \n",
    "        return error_msg\n",
    "\n",
    "# Test della funzione con gestione errori migliorata\n",
    "print(\"Test di comunicazione con Ollama...\")\n",
    "test_response = chat_with_ollama(\"Ciao! Come stai?\")\n",
    "print(f\"Risposta di Ollama: {test_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5c4bde",
   "metadata": {},
   "source": [
    "## 4. Simulazione Client MCP\n",
    "\n",
    "Creiamo un client simulato che pu√≤ utilizzare i tool del server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e77fd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool disponibili: ['calculator', 'text_analyzer']\n"
     ]
    }
   ],
   "source": [
    "class MCPClient:\n",
    "    def __init__(self, server):\n",
    "        self.server = server\n",
    "        self.available_tools = []\n",
    "    \n",
    "    async def initialize(self):\n",
    "        \"\"\"Inizializza il client e ottiene la lista dei tool\"\"\"\n",
    "        try:\n",
    "            self.available_tools = await self.server.get_tools()\n",
    "            print(f\"Tool disponibili: {[tool.name for tool in self.available_tools]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Errore nell'inizializzazione del client: {e}\")\n",
    "            # Fallback: usa la lista dei tool direttamente\n",
    "            self.available_tools = self.server.tools_list\n",
    "            print(f\"Tool disponibili (fallback): {[tool.name for tool in self.available_tools]}\")\n",
    "    \n",
    "    async def call_tool(self, tool_name: str, arguments: Dict[str, Any]):\n",
    "        \"\"\"Chiama un tool specifico\"\"\"\n",
    "        try:\n",
    "            result = await self.server.call_tool_method(tool_name, arguments)\n",
    "            return result[0].text if result else \"Nessun risultato\"\n",
    "        except Exception as e:\n",
    "            return f\"Errore nella chiamata al tool: {str(e)}\"\n",
    "    \n",
    "    def parse_tool_call(self, llm_response: str):\n",
    "        \"\"\"Analizza la risposta del LLM per identificare chiamate ai tool\"\"\"\n",
    "        if \"TOOL_CALL:\" in llm_response:\n",
    "            try:\n",
    "                # Estrae il nome del tool e gli argomenti\n",
    "                parts = llm_response.split(\"TOOL_CALL:\")[1].strip()\n",
    "                tool_name = parts.split(\" \")[0]\n",
    "                args_str = parts.split(\" \", 1)[1] if \" \" in parts else \"{}\"\n",
    "                arguments = json.loads(args_str)\n",
    "                return tool_name, arguments\n",
    "            except Exception as e:\n",
    "                print(f\"Errore nel parsing della chiamata tool: {e}\")\n",
    "                return None, None\n",
    "        return None, None\n",
    "\n",
    "# Inizializzazione del client\n",
    "mcp_client = MCPClient(mcp_server)\n",
    "await mcp_client.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4a603a",
   "metadata": {},
   "source": [
    "## 5. Workflow Completo\n",
    "\n",
    "Ora combiniamo tutto per creare un workflow completo che usa Ollama con i tool MCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f521619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 1: Calcolo matematico ===\n",
      "ü§ñ Utente: Quanto fa 15 * 23 + 7?\n",
      "üí≠ LLM risposta: TOOL_CALL: calculator {\"value\": \"41875\"}\n",
      "üîß Chiamata tool: calculator con argomenti: {'value': '41875'}\n",
      "üìä Risultato tool: Errore nel calcolo: 'expression'\n",
      "‚úÖ Risposta finale: Per risolvere questo problema, possiamo following i passaggi:\n",
      "\n",
      "1.  Calcolare il prodotto del numero 15 per cui siamo chiedendo il risultato di 15 * 23.\n",
      "2.  Aggiungere la somma di 7 al risultato del calcolo precedente.\n",
      "\n",
      "Calcolando i due risultati:\n",
      "\n",
      "15 * 23 = 345\n",
      "\n",
      "Aggiungiamo poi 7 ai risultati precedentemente calcolati:\n",
      "\n",
      "345 + 7 = 352\n",
      "\n",
      "=== Test 2: Analisi testo ===\n",
      "ü§ñ Utente: Puoi analizzare questo testo: 'Il machine learning √® una branca dell'intelligenza artificiale. Permette ai computer di imparare dai dati.'?\n",
      "üí≠ LLM risposta: TOOL_CALL: text_analyzer {\"argomento\": \"testo per analisi di linguaggio naturale\"}\n",
      "üîß Chiamata tool: text_analyzer con argomenti: {'argomento': 'testo per analisi di linguaggio naturale'}\n",
      "üìä Risultato tool: Errore nella chiamata al tool: 'text'\n",
      "‚úÖ Risposta finale: Ciao!\n",
      "\n",
      "Capisco che tu hai chiesto l'analisi di un testo specifico, ma non ho ricevuto la domanda corretta.\n",
      "\n",
      "Tuttavia, posso provare a fornirti una risposta generale all'intera domanda.\n",
      "\n",
      "Il testo \"Il machine learning √® una branca dell'intelligenza artificiale. Permette ai computer di imparare dai dati\" sembra essere un concetto molto semplice e chiaro. Ecco alcune considerazioni generali:\n",
      "\n",
      "* **Intelligenza Artificiale (IA)**: La IA √® un campo che si occupa di sviluppare macchine che possano pensare, imparare e apprendere in modo autonomo.\n",
      "* **Machine Learning (ML)**: ML √® una tecnologia specifica dell'IA che consiste nell'applicazione di algoritmi per permettere ai computer di imparare dai dati. I modelli di ML vengono utilizzati per prevedere eventi futuri, analizzare dati e migliorare la comprensione del mondo.\n",
      "* **Branca dell'IA**: Il termine \"branca\" pu√≤ essere interpretato in diversi modi. In questo caso, potrebbe riferirsi alla specifica applicazione di ML, ovvero il machine learning.\n",
      "\n",
      "Ora, su cui devi provare a capire se il tool text_analyzer ti ha effettuato una chiamata al metodo TextAnalysis come richiesto?\n",
      "\n",
      "Se √® cos√¨, probabilmente avresti dovuto scrivere qualcosa come: \"Il testo ' Il machine learning √® una branca dell'intelligenza artificiale. Permette ai computer di imparare dai dati' non ha senso, perch√© il termine 'machine learning' √® gi√† utilizzato in modo molto pi√π ampio.\"\n",
      "\n",
      "Ora ti dar√≤ un esempio completo e chiaro:\n",
      "\n",
      "\"La tecnologia della machine learning √® una branca dell'intelligenza artificiale. Permette ai computer di imparare dai dati. Come funziona? E in che campo si applica?\"\n",
      "\n",
      "=== Test 3: Domanda generale ===\n",
      "ü§ñ Utente: Spiegami cosa sono i protocolli di comunicazione.\n",
      "üí≠ LLM risposta: TOOL_CALL: comunicazione\n",
      "\n",
      "C'√® un modello per la comunicazione che pu√≤ utilizzare questi tool per aiutare l'utente a trovare le risposte alle sue domande. I protocolli della comunicazione si riferiscono alla sequenza di azioni, messaggi o interazioni tra persone (o oggetti) che si svolgono in un ambiente sociale.\n",
      "\n",
      "Questi protocolli possono essere suddivisi in diversi livelli:\n",
      "\n",
      "* **Protocollo della comunicazione** (level 1): rappresenta la struttura di base dei protocolli, inclusi il tipo di messaggio, il destinatario, l'argomento e i contenuti.\n",
      "* **Protocollo di interazione** (level 2): rappresenta le azioni che si svolgono tra le persone durante la comunicazione, come esempio:\n",
      " + \"Ecco il mio calcolatore.\"\n",
      " + \"Quale √® il valore del tuo protocollo?\"\n",
      "* **Protocollo di risposta** (level 3): rappresenta la reazione dell'utente a un messaggio o azione precedente.\n",
      "\n",
      "Questa struttura pu√≤ essere ulteriormente suddivisa in livelli aggiuntivi, come:\n",
      "* Protocollo della comunicazione teorica\n",
      "* Protocollo della comunicazione pratica\n",
      "\n",
      "Utilizzando questi protocolli di comunicazione, l'utente pu√≤ comprendere meglio la struttura dei suoi messaggi e azioni, e utilizzare i tool per aiutare a risolvere le sue domande.\n",
      "Errore nel parsing della chiamata tool: Expecting value: line 1 column 1 (char 0)\n",
      "üí¨ Risposta diretta (senza tool): TOOL_CALL: comunicazione\n",
      "\n",
      "C'√® un modello per la comunicazione che pu√≤ utilizzare questi tool per aiutare l'utente a trovare le risposte alle sue domande. I protocolli della comunicazione si riferiscono alla sequenza di azioni, messaggi o interazioni tra persone (o oggetti) che si svolgono in un ambiente sociale.\n",
      "\n",
      "Questi protocolli possono essere suddivisi in diversi livelli:\n",
      "\n",
      "* **Protocollo della comunicazione** (level 1): rappresenta la struttura di base dei protocolli, inclusi il tipo di messaggio, il destinatario, l'argomento e i contenuti.\n",
      "* **Protocollo di interazione** (level 2): rappresenta le azioni che si svolgono tra le persone durante la comunicazione, come esempio:\n",
      " + \"Ecco il mio calcolatore.\"\n",
      " + \"Quale √® il valore del tuo protocollo?\"\n",
      "* **Protocollo di risposta** (level 3): rappresenta la reazione dell'utente a un messaggio o azione precedente.\n",
      "\n",
      "Questa struttura pu√≤ essere ulteriormente suddivisa in livelli aggiuntivi, come:\n",
      "* Protocollo della comunicazione teorica\n",
      "* Protocollo della comunicazione pratica\n",
      "\n",
      "Utilizzando questi protocolli di comunicazione, l'utente pu√≤ comprendere meglio la struttura dei suoi messaggi e azioni, e utilizzare i tool per aiutare a risolvere le sue domande.\n"
     ]
    }
   ],
   "source": [
    "async def complete_workflow(user_input: str):\n",
    "    \"\"\"\n",
    "    Workflow completo: LLM + MCP Tools\n",
    "    \"\"\"\n",
    "    print(f\"ü§ñ Utente: {user_input}\")\n",
    "    \n",
    "    # 1. Ottieni risposta da Ollama con informazioni sui tool\n",
    "    llm_response = chat_with_ollama(user_input, tools_available=True)\n",
    "    print(f\"üí≠ LLM risposta: {llm_response}\")\n",
    "    \n",
    "    # 2. Controlla se il LLM vuole usare un tool\n",
    "    tool_name, arguments = mcp_client.parse_tool_call(llm_response)\n",
    "    \n",
    "    if tool_name:\n",
    "        print(f\"üîß Chiamata tool: {tool_name} con argomenti: {arguments}\")\n",
    "        \n",
    "        # 3. Esegui il tool\n",
    "        tool_result = await mcp_client.call_tool(tool_name, arguments)\n",
    "        print(f\"üìä Risultato tool: {tool_result}\")\n",
    "        \n",
    "        # 4. Ottieni una risposta finale dal LLM con il risultato del tool\n",
    "        final_prompt = f\"L'utente ha chiesto: {user_input}\\nHo usato il tool {tool_name} e il risultato √®: {tool_result}\\nFornisci una risposta completa e utile.\"\n",
    "        final_response = chat_with_ollama(final_prompt)\n",
    "        print(f\"‚úÖ Risposta finale: {final_response}\")\n",
    "    else:\n",
    "        print(f\"üí¨ Risposta diretta (senza tool): {llm_response}\")\n",
    "\n",
    "# Test del workflow completo\n",
    "print(\"=== Test 1: Calcolo matematico ===\")\n",
    "await complete_workflow(\"Quanto fa 15 * 23 + 7?\")\n",
    "\n",
    "print(\"\\n=== Test 2: Analisi testo ===\")\n",
    "await complete_workflow(\"Puoi analizzare questo testo: 'Il machine learning √® una branca dell'intelligenza artificiale. Permette ai computer di imparare dai dati.'?\")\n",
    "\n",
    "print(\"\\n=== Test 3: Domanda generale ===\")\n",
    "await complete_workflow(\"Spiegami cosa sono i protocolli di comunicazione.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fe9492",
   "metadata": {},
   "source": [
    "## 6. Test Interattivo\n",
    "\n",
    "Creiamo un semplice loop interattivo per testare il sistema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c202afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Chat MCP + Ollama avviata!\n",
      "Tool disponibili: calculator, text_analyzer\n",
      "Digita 'quit' per uscire.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async def interactive_chat():\n",
    "    \"\"\"\n",
    "    Chat interattiva con MCP + Ollama\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Chat MCP + Ollama avviata!\")\n",
    "    print(\"Tool disponibili: calculator, text_analyzer\")\n",
    "    print(\"Digita 'quit' per uscire.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"Tu: \")\n",
    "            if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "                print(\"üëã Chat terminata!\")\n",
    "                break\n",
    "            \n",
    "            if user_input.strip():\n",
    "                await complete_workflow(user_input)\n",
    "                print(\"-\" * 50)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nüëã Chat terminata!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Errore: {e}\")\n",
    "\n",
    "# Per avviare la chat interattiva, decommentare la riga seguente:\n",
    "await interactive_chat()\n",
    "\n",
    "print(\"‚úÖ Notebook completato! Decommentare l'ultima riga per avviare la chat interattiva.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca85bdf",
   "metadata": {},
   "source": [
    "## Risoluzione Problemi\n",
    "\n",
    "Se riscontri errori, prova i seguenti comandi dal terminale:\n",
    "\n",
    "### Windows (PowerShell)\n",
    "```powershell\n",
    "# Attiva il virtual environment\n",
    "source .venv/Scripts/activate.ps1\n",
    "\n",
    "# Verifica Ollama\n",
    "ollama --version\n",
    "ollama list\n",
    "ollama serve  # Se il servizio non √® attivo\n",
    "\n",
    "# Scarica il modello se necessario\n",
    "ollama pull llama3.2:1b\n",
    "```\n",
    "\n",
    "### Verifica installazione\n",
    "```bash\n",
    "python -c \"import ollama; print('Ollama library OK')\"\n",
    "python -c \"import mcp; print('MCP library OK')\"\n",
    "```\n",
    "\n",
    "## Conclusioni\n",
    "\n",
    "Questo notebook dimostra:\n",
    "\n",
    "1. **Setup MCP Server**: Creazione di un server con tool personalizzati\n",
    "2. **Integrazione Ollama**: Uso del modello llama3.2:1b locale\n",
    "3. **Workflow completo**: Combinazione di LLM e tool MCP\n",
    "4. **Tool disponibili**:\n",
    "   - `calculator`: per calcoli matematici\n",
    "   - `text_analyzer`: per analisi di testi\n",
    "\n",
    "### Prossimi passi:\n",
    "- Aggiungere pi√π tool personalizzati\n",
    "- Implementare persistenza dei dati\n",
    "- Migliorare il parsing delle chiamate ai tool\n",
    "- Aggiungere validazione degli input"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
