# System Card: MS02-ANALYZER - Document Analysis & Entity Extraction

**Status**: ‚úÖ FINAL | **Version**: 1.0 | **Date**: 21 Nov 2025 | **AI Act**: EU 2024/1689 (Articles 11-13)

---

## 1. Model Identity

- **Name**: MS02-ANALYZER - Semantic Analysis & Entity Extraction
- **ID**: ms02-analyzer-v1.8.2 | **Risk Level**: üü† MEDIUM-RISK
- **Type**: Multi-task NLP (Entity Extraction, Semantic Analysis, Sentiment Analysis)
- **Framework**: spaCy 3.5 + Transformers (DistilBERT + custom transformers)
- **Size**: 340 MB | **Parameters**: 8.5M | **License**: Apache 2.0
- **Provider**: ZenIA / Interzen | **Release**: 2025-11-21
- **Lineage**: Fine-tuned on Italian PA documents; base model: spacy-it-core-news-lg

---

## 2. Intended Use & Restrictions

### Primary Use Case
Extract and classify named entities (persons, organizations, locations, amounts, dates) from Italian PA documents to support document analysis, template population, and semantic search.

### Approved Use
- ‚úÖ Entity extraction from documents
- ‚úÖ Semantic analysis for content understanding
- ‚úÖ Information extraction for database population
- ‚úÖ Human-reviewed extractions only

### Prohibited Use
- ‚ùå Automated PII extraction without governance
- ‚ùå Cross-linking person entities for surveillance
- ‚ùå Automated profiling based on extracted entities

### Known Limitations
- **Language**: Italian primary (96%+ accuracy); English ~80% accuracy
- **Entity Types**: 12 types (Person, Organization, Location, Date, Amount, Fiscal_Code, Email, Phone, Contract_Type, Risk_Level, Department, Other)
- **Performance**: Best on formal documents (contracts, invoices); variable on informal correspondence
- **OCR Dependency**: Requires OCR quality > 85%

---

## 3. Training Data

- **Dataset**: 32,000 Italian PA documents, manually annotated with entities
- **Annotation Quality**: Inter-annotator agreement 94%
- **Data Split**: 70% training, 15% validation, 15% test
- **Entity Distribution**: Balanced across all 12 entity types
- **GDPR**: All PII removed post-annotation; anonymized dataset retained

---

## 4. Performance Metrics

### Entity Recognition Performance (Test Set)

| Entity Type | Precision | Recall | F1-Score | Support |
|-----------|-----------|--------|----------|---------|
| PERSON | 0.92 | 0.88 | 0.90 | 2,400 |
| ORGANIZATION | 0.89 | 0.85 | 0.87 | 1,800 |
| LOCATION | 0.85 | 0.82 | 0.83 | 1,200 |
| DATE | 0.96 | 0.94 | 0.95 | 2,000 |
| AMOUNT | 0.91 | 0.89 | 0.90 | 1,600 |
| FISCAL_CODE | 0.98 | 0.97 | 0.975 | 1,400 |
| EMAIL | 0.97 | 0.96 | 0.965 | 800 |
| PHONE | 0.94 | 0.92 | 0.93 | 600 |

**Overall**: Macro F1 = 0.908 | Weighted F1 = 0.913

### Semantic Analysis Performance
- Semantic similarity (cosine): 0.89 correlation with human similarity ratings
- Topic modeling: 84% coherence score
- Sentiment detection: 87% accuracy (3-way classification)

---

## 5. Fairness Assessment

### Bias Analysis
- **Language Fairness**: Italian 96% ‚Üí English 80% (acceptable; Italian is primary)
- **Entity Type Fairness**: Performance uniform across entity types (Macro F1 std dev: 0.04)
- **Document Complexity**: Simple docs 94% F1 ‚Üí Complex docs 87% F1 (6pp gap acceptable)
- **Organization Type Fairness**: Central PA 91.5% ‚Üí Local PA 89.8% (1.7pp disparity ‚úÖ)

### Known Disparities
- ‚ö†Ô∏è Reduced performance on non-standard document formats
- ‚ö†Ô∏è Lower accuracy for uncommon entity names (< 95% accuracy)
- **Mitigation**: Manual review for low-confidence extractions (< 0.75)

---

## 6. Environmental Impact

- **Training Carbon**: 8.3 kg CO2e (12 hours GPU training)
- **Annual Inference**: 342 kg CO2e/year (baseline)
- **With Optimization**: 82 kg CO2e/year (76% reduction via batching + caching)
- **Mitigation**: Model distillation implemented (18% inference speedup)

---

## 7. Human Oversight

### Explainability
- **Confidence Scores**: Per-entity confidence (0.0-1.0)
- **Highlighting**: Attention visualization showing which text parts influenced entity recognition
- **Alternatives**: Top-3 alternative entity types provided for ambiguous extractions

### Human Review
- **Automatic Escalation**: Confidence < 0.70 ‚Üí manual review queue
- **Override Capability**: PA staff can correct/override all extractions
- **Audit Trail**: All corrections logged for feedback

### Monitoring
- **Real-Time Metrics**: Entity recognition accuracy per type
- **Drift Detection**: Alert if accuracy drops > 3%
- **Retraining**: Quarterly with new annotated examples

---

## 8. Risk Assessment

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|-----------|
| Incorrect person entity extraction | Medium | High | Confidence threshold + manual review |
| False positive organization detection | Medium | Medium | Validation against company registry |
| PII exposure in extracted entities | Low | Critical | PII governance policy + audit |
| Concept drift over time | Medium | Medium | Quarterly retraining + monitoring |

**Residual Risk**: ACCEPTED with monitoring and manual review process

---

## 9. Approvals

- ‚úÖ Technical Review: [Marco Rossi] - 2025-11-21
- ‚úÖ Security Review: [Anna Bianchi] - 2025-11-21
- ‚úÖ Compliance Review: [Giovanni Verdi] - 2025-11-21
- ‚è≥ Executive Approval: PENDING

---

## 10. References

- spaCy Documentation: https://spacy.io
- Transformers: https://huggingface.co/docs/transformers
- Compliance: See [COMPLIANCE-MAPPING-AI-ACT.md](../../COMPLIANCE-MAPPING-AI-ACT.md)

---

**System Card Version**: 1.0 | **Valid Until**: 21 May 2026 | **Next Review**: Q1 2026 or on model update
